<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Japan Rook Meetup #2 | jkkitakita</title><meta property="og:title" content="Japan Rook Meetup #2 - jkkitakita"><meta property="og:description" content="Japan Rook Meetup #2"><meta property="og:url" content="https://blog.jkkitakita.dev/post/202003291728/"><meta property="og:site_name" content="jkkitakita"><meta property="og:type" content="article"><meta property="og:image" content="https://www.gravatar.com/avatar/23c25c7545ad2fdbdbab1f9201bf96b7?s=256"><meta property="article:author" content="https://facebook.com/jkkitakita"><meta property="article:section" content="Post"><meta property="article:tag" content="rook"><meta property="article:tag" content="event"><meta property="article:published_time" content="2020-03-29T17:28:37+09:00"><meta property="article:modified_time" content="2020-03-29T17:28:37+09:00"><meta name=twitter:card content="summary"><meta name=twitter:site content="@jkkitakita"><meta name=twitter:creator content="@jkkitakita"><link href=https://blog.jkkitakita.dev/index.xml rel=alternate type=application/rss+xml title=jkkitakita><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://blog.jkkitakita.dev/css/custom.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://blog.jkkitakita.dev/post/202003291728/><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"></head><body><section class=section><div class=container><nav id=nav-main class=nav><div id=nav-name class=nav-left><a id=nav-anchor class=nav-item href=https://blog.jkkitakita.dev><h1 id=nav-heading class="title is-4">jkkitakita</h1></a></div><div class=nav-right><nav id=nav-items class="nav-item level is-mobile"><a class=level-item aria-label=github href=https://github.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></i></span></a><a class=level-item aria-label=facebook href=https://facebook.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></i></span></a><a class=level-item aria-label=twitter href=https://twitter.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></i></span></a><a class=level-item aria-label=instagram href=https://instagram.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></i></span></a><a class=level-item aria-label=email href=mailto:jkkitakita@gmail.com target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></i></span></a><a class=level-item aria-label=linkedin href=https://linkedin.com/in/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path stroke-width="1.8" d="m5.839218 4.101561c0 1.211972-.974141 2.194011-2.176459 2.194011S1.4863 5.313533 1.4863 4.101561c0-1.211094.974141-2.194011 2.176459-2.194011s2.176459.982917 2.176459 2.194011zm.017552 3.94922h-4.388022v14.04167H5.85677V8.050781zm7.005038.0H8.501869v14.04167h4.360816v-7.370999c0-4.098413 5.291077-4.433657 5.291077.0v7.370999h4.377491v-8.89101c0-6.915523-7.829986-6.66365-9.669445-3.259423V8.050781z"/></svg></i></span></a></nav></div></nav><nav class=nav></nav></div><script src=/js/navicon-shift.js></script></section><section class=section><div class=container><div class="subtitle tags is-6 is-pulled-right"><a class="subtitle is-6" href=/tags/rook/>#rook</a>
| <a class="subtitle is-6" href=/tags/event/>#event</a></div><h2 class="subtitle is-6">March 29, 2020</h2><h1 class=title>Japan Rook Meetup #2</h1><div class=content><h2 id=はじめに>はじめに</h2><p>今回はリモートでの開催だった<a href=https://rook.connpass.com/event/160657/>Japan Rook Meetup #2</a>に関するブログ枠です。基本的に、意見ほぼなしのまとめです。GlusterFSとかはなんとなく触ったことがあるのですが、RookとかCephとかはほぼ触ったことがなかったので、新鮮に勉強させていただきました。何かここが違うなど問題があれば、twitter等でご指摘いただければと思います。</p><h2 id=アジェンダ>アジェンダ</h2><p><img src=/images/agenda_rook_meetup_2.png alt=agenda_rook_meetup_2></p><h2 id=ハッシュタグ>ハッシュタグ</h2><p><a href="https://twitter.com/search?q=%23k8sjp">#k8sjp</a></p><h2 id=youtube>Youtube</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/xSLIrfkBKv4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=内容>内容</h2><h3 id=rook-基礎バージョンアップ-rev4thttpstwittercomrev4t>Rook 基礎・バージョンアップ <a href=https://twitter.com/rev4t>@rev4t</a></h3><hr><iframe src=//www.slideshare.net/slideshow/embed_code/key/8Fiu2FQopxr41n width=595 height=376 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe><h4 id=そもそもどういう時に利用する物なのか>そもそもどういう時に利用する物なのか</h4><p>Kubernetesで外部ストレージを利用したい時</p><h4 id=解決できること>解決できること</h4><p>k8s上でStorage Operationを実現。storageは運用が大変だが、その自動化ができる。</p><h4 id=rookとは>Rookとは</h4><p>CNCF。Operator + Custom Resouceでk8s拡張可能。<br>また、沢山のStorageシステムに対応したフレームワーク</p><ul><li>Ceph（Stable）</li><li>EdgeFS（Stable）</li><li>CockroachDB</li><li>Cassandra</li><li>NFS</li><li>YugabyteDB</li><li>minio</li></ul><h4 id=構成要素>構成要素</h4><ul><li>Rook Operator<br>実際にストレージのデプロイ・管理をしてくれるもの</li><li>Rook Discover<br>ストレージノードの変更等を検知して、Operatorに伝える etc&mldr;</li></ul><h4 id=version-up>Version Up</h4><h5 id=rookmajor-version>Rook（major version）</h5><ol><li>前提として、Cephが正常に起動していることを確認</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># cephのステータス確認</span>
ceph status
</code></pre></div><ol start=2><li>RookのControl planeが正常に稼働していることを確認</li><li>CRDとRBACの更新</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g.</span>
<span style=color:#75715e># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-apply.yaml</span>

kubectl apply -f upgrade-from-v1.1-apply.yaml
</code></pre></div><ol start=4><li>fuse or rdb-ndbを使っている場合、Cephfs plugin。rdb pluginの update strategyを onDeleteに変更</li><li>Rook operatorのupdate</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2.5へ</span>
kubectl -n $ROOK_SYSTEM_NAMESPACE set image deploy/rook-ceph-oeprator rook-ceph-operator<span style=color:#f92672>=</span>rook/ceph:v1.2.5
</code></pre></div><ol start=6><li>更新されるまで、しばらく待つ</li><li>fuse or rdb-ndbを使っている場合、CSI driverのpodをdeleteすることで更新させる</li><li>最後に、CRDを更新</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g.</span>
<span style=color:#75715e># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-crds.yaml</span>

kubectl apply -f upgrade-from-v1.1-crds.yaml
</code></pre></div><h5 id=rookminor-version>Rook（minor version）</h5><ol><li>imageのversionを更新するだけ。</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2.5へ</span>
kubectl -n $ROOK_SYSTEM_NAMESPACE set image deploy/rook-ceph-oeprator rook-ceph-operator<span style=color:#f92672>=</span>rook/ceph:v1.2.5
</code></pre></div><p>（注）Cephのdelete/createが伴う場合があるそう。</p><p><img src="https://image.slidesharecdn.com/rook-meetup-2-sogabe-rev1-200327104438/95/rook-31-638.jpg?cb=1585306044" alt=rook-minor-upgrade></p><h5 id=cephv1424---v1427>Ceph（v14.2.4 -> v14.2.7）</h5><ol><li>imageのversionを更新する。（imageを更新すれば、あとはRook Operatorがやってくれる。）</li><li>完了するまで待つ。</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># cephのステータス確認して、HEALTH_OKかどうか</span>
ceph status

cluster: id: 3casefd74-edfg-2352-b54j-51jk12tj10
health: HEALTH_OK
</code></pre></div><h5 id=external-cluster>External Cluster</h5><p>既存のCeph Clusterに対して、Rookを用いたk8sとの連携ができる。</p><h6 id=メリット>メリット</h6><ol><li>CSI Driverの運用管理のみをk8sで実施する形になり、k8s側の運用コストを下げつつ、責任分界点を分けられる</li><li>Rook Version Upの際に、Ceph Clusterに与えるサービス影響を完全に排除できる</li><li>k8s networkingによる性能低下の影響をうけなくなる。</li></ol><h6 id=デメリット>デメリット</h6><ol><li>Rookの他に、Cephのデプロイツールの導入をする必要が出てくる</li></ol><h6 id=利用例>利用例</h6><p>Rook + ceph-ansible<br><a href=https://github.com/ceph/ceph-ansible>https://github.com/ceph/ceph-ansible</a></p><h4 id=summary>Summary</h4><ul><li>長期的な運用で、Storage Backendだけでなく、Rook Operaotr自体の運用もしなくてはならない。</li><li>Cephのversionがすごく楽になる。</li><li>k8s側の運用をシンプルにしたい場合は、ceph-ansible + Rook external clusterが良さそう。</li></ul><h4 id=質疑応答>質疑応答</h4><blockquote><p>Q. Ceph OSDのupdateで、nodeを変更しない以外のパターン以外で、新規のnodeにmigrationをかけることは可能か？<br>A. 検証していないが、多分できると思っている。</p></blockquote><blockquote><p>Q. Control Planのupgradeする時に、実際にデータアクセスに影響があるということでしょうか？<br>A. Ceph OSDのpodを再起動されることになるので、クライアントからのリクエストのlatencyが高くなるだろうと思っている。影響の少ない時間帯で実施するなどの対応が必要だろうと思っている。</p></blockquote><h3 id=rookceph-upstream最新状況-satoru_takeuchihttpstwittercomsatoru_takeuchi>Rook/Ceph upstream最新状況 <a href=https://twitter.com/satoru_takeuchi>@satoru_takeuchi</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=目次>目次</h4><ol><li>前提知識: OSD作成方法</li><li>Rook最新安定版の新機能</li><li>Rook次期安定版の開発状況</li><li>Rookを取り巻く状況</li></ol><h4 id=1-前提知識-osd作成方法>1. 前提知識: OSD作成方法</h4><hr><p>OSDの作成方法には大きく２つの方法があるとのこと</p><h5 id=11-従来の方法>1.1. 従来の方法</h5><p>OSDの設定にノード上のデバイスを直接指定する</p><ul><li>これだと、Rook/Ceph cluster管理者にハードウェア構成の知識が必要</li><li>さらに、デバイス名直接指定しているので、自前でclusterを構築する必要がある</li></ul><h5 id=12-osd-on-pvc-v11>1.2. OSD on PVC v1.1~</h5><p>OSDの設定にPVC templateを指定する。それを受けて、CSI driverがPVを作成する。Cluster管理者はOSD podがどこにあるかを気にしなくて良いさらに、デバイスの管理もしなくて良くなる。</p><h4 id=2-rook最新安定版の新機能>2. Rook最新安定版の新機能</h4><hr><h5 id=21-osd-on-pvcでlvサポート>2.1. OSD on PVCでLVサポート</h5><ul><li>OSD on PVCでロジカルボリューム（LV）対応した（サイボウズ作）</li></ul><h6 id=目的背景>目的・背景</h6><ol><li>ローカルのNVMe SSD上にOSDを作成し、OSDを束ねて、Ceph Clusterを構築し、ブロックデバイス（RDB？）を提供したい</li><li>管理コスト削減のため、PVをdynamic provisioningしたい。pvが欲しくなったら、dynamicなstorageが欲しい。</li></ol><h6 id=既存の課題>既存の課題</h6><p>以下の条件を満たすCSIドライバがない</p><ul><li>ローカルボリューム</li><li>dynamic provisioning</li><li>実用レベルのものがない</li></ul><p>つまり、デバイス追加は手作業でPV作成する必要がある。</p><h6 id=topolvm>TopoLVM</h6><ul><li>PVCの変更を検知して、PVをdynamiv provisioningしてくれるやつ？</li></ul><h5 id=22-従来方式でudev-persistent-nameをサポート>2.2. 従来方式でudev persistent nameをサポート</h5><p>v1.1以前では、/dev直下は気付いたら変わりうる。e.g. sda -> sdb<br>v1.2以降では、新機能devicePathFilter（サイボウズ作）</p><ul><li>こうすると、/dev直下のdevice nameは変わりうるのだが、by-path名は変更されない</li><li>そのため、データ破壊を防げる</li></ul><h5 id=23-その他変更>2.3. その他変更</h5><ol><li>cech-crash collector</li></ol><ul><li>daemonのクラッシュ情報をCephクラスタに保存できるので、トラブルシューティングに効果的</li><li>デフォルトで有効</li></ul><ol start=2><li>FileStore OSDがobsolete（既存のFileStore OSDはサポートし続ける）</li></ol><h4 id=3-rook次期安定版の開発状況>3. Rook次期安定版の開発状況</h4><hr><h5 id=31-failure-domainをまたいだosdの均等分散配置>3.1. Failure domainをまたいだOSDの均等分散配置</h5><p>k8sのTopologySpreadConstraints機能のサポート（OSD on PVCに必須）</p><h6 id=topologyspreadconstraintsの目的>TopologySpreadConstraintsの目的</h6><ol><li>ラック障害耐性</li><li>ノード障害耐性</li></ol><h6 id=これまでの課題>これまでの課題</h6><p>OSD podの偏りが生まれてしまい、ラックの障害耐性がない場合が有り得る。次期安定版から、対応予定で、これでようやくOSD on PVCが使えるものとなると思っている。</p><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide=20 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=32-osd-on-lv-backed-pvcのテスト追加>3.2. OSD on LV-backed PVCのテスト追加</h5><ol><li>RookはPRマージ時、テスト全パスが必須。</li></ol><p>しかし、3つの壁が。。。</p><h6 id=壁1-masterでosd-on-lv-backed-pvcが動かない>壁1: masterでOSD on LV-backed PVCが動かない</h6><ol><li>このPRでリグレッションが発生
<a href=https://github.com/rook/rook/pull/4435>https://github.com/rook/rook/pull/4435</a></li><li>Issue発行
<a href=https://github.com/rook/rook/issues/5075>https://github.com/rook/rook/issues/5075</a></li><li>CI重要！！！！</li></ol><h6 id=壁2-テストがテスト環境を破壊>壁2: テストがテスト環境を破壊</h6><ol><li>テスト冒頭で全LV/VG/PVを強制削除</li><li>PRマージ待ち
<a href=https://github.com/rook/rook/pull/4966>https://github.com/rook/rook/pull/4966</a></li></ol><h6 id=壁3-ローカル環境でテストがパスしない>壁3: ローカル環境でテストがパスしない</h6><p>調査中</p><h5 id=33-filestoreの新規作成が不可能に>3.3. FileStoreの新規作成が不可能に</h5><p>obsoluteではなく、不可能へ。</p><h5 id=34-ストレージプロバイダ>3.4. ストレージプロバイダ</h5><ol><li>新しいストレージプロバイダなし</li><li>minioのコードが削除（メンテされていないから。）</li></ol><h5 id=35-その他の課題>3.5. その他の課題</h5><p>コア部分と前ストレージプロバイダが同じリポジトリ管理しているので、開発がしにくい</p><ul><li>kubernetesとstorage pluginを別リポジトリを分けた話と同じ感じ。</li></ul><h5 id=36-今後の貢献予定>3.6. 今後の貢献予定</h5><ol><li>テストの充実化</li><li>暗号化デバイス上のOSD</li><li>バグ解決</li></ol><h4 id=4-rookを取り巻く状況>4. Rookを取り巻く状況</h4><hr><h5 id=41-プロジェクトの成熟度>4.1. プロジェクトの成熟度</h5><ul><li>現在は、Incubating</li><li>もうちょっとで、卒業するかも？
<a href=https://github.com/cncf/toc/pull/366>https://github.com/cncf/toc/pull/366</a></li></ul><h5 id=42-rookcephを使う製品>4.2. Rook/Cephを使う製品</h5><ul><li><a href=https://blog.openshift.com/introducing-openshift-container-storage-4-2/>Red Hat OpenShift Container Storage4（GA・2020-01-15）</a></li><li><a href=https://www.suse.com/c/ceph-on-kubernetes-tech-preview/>Containerized SUSE Enterprise Storage on CaaSP（Tecknical preview）</a></li></ul><h4 id=5-参考リンク>5. 参考リンク</h4><ul><li><a href=https://blog.cybozu.io/entry/2019/12/03/114746>ストレージオーケストレーターRookへのサイボウズのコミット方針</a></li><li><a href=https://blog.cybozu.io/entry/2019/11/08/090000>Kubernetesでローカルストレージを有効活用しよう</a></li><li><a href="https://docs.google.com/presentation/d/1mMPYMDC4JMGWhoL3FzFgeasSLJepNwYMfwQD-T_gET4/edit#slide=id.g3a79217937_0_104">Rook - Graduation Proposal</a></li><li>賢く「散らす」ための Topology Spread Constraints<div class=embed><script async class=speakerdeck-embed data-id=61215f14bf534bce8ee5726a2ce243dd data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div></li></ul><h4 id=質疑応答-1>質疑応答</h4><blockquote><p>Q. TopoLVMに関して、LVMのレイヤーをdeclaretiveなやり方で、VG？を作ってくれるものという認識であっていますでしょうか？</p><p>A. はい。その通りです。Storage ClassでTopoLVMのものを指定していただければ、LVを勝手に切ってくれて、それをブロックデバイス、ないし、ファイルシステムとして利用できるものです。</p></blockquote><blockquote><p>Q. FileStoreがobsoluteになって、BlueStoreへの移行がこれから出てくると思うのだが、マイグレーションパスの開発は進んでいるのでしょうか？</p><p>A. 現在、ドキュメント整備中です。基本的には、そのドキュメントがみてやっていただければ良いようになっていく予定です。</p></blockquote><h3 id=rook-cephでexternal-clusterを利用する-futa_0203httpstwittercomfuta_0203>Rook-CephでExternal Clusterを利用する <a href=https://twitter.com/FUTA_0203>@FUTA_0203</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=本日お話しすること>本日お話しすること</h4><ol><li>Rook-Ceph External Clusterの概要</li><li>Rook-Ceph External Clusterの利用方法</li><li>Rook-Ceph External Cluster利用時の注意点</li></ol><h5 id=1-rook-ceph-external-clusterの概要>1. Rook-Ceph External Clusterの概要</h5><hr><p>Rook-Cephを構築したクラスター外に存在する、Cephクラスターのストレージリソースを利用すること</p><ul><li>local: k8s（Rook） cluster</li><li>external: Ceph cluster</li></ul><h6 id=本機能の背景>本機能の背景</h6><ol><li>Rook ver 1.1から利用可能</li><li>基本的には、k8s内のstorageを利用する想定だった</li><li>しかし、そうではない、ユースケースもあったため。</li></ol><p>そうではないユースケースとは&mldr;</p><ul><li>既存Ceph Clusterが存在する</li><li>1つのCephクラスターのリソースを複数k8sで利用したい</li><li>単純にstorageを分離したい</li></ul><h6 id=rook-cephクラスター構築通常>Rook-Cephクラスター構築（通常）</h6><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2</span>
git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
cd cluster/examples/kubernetes/ceph

1. kubectl create -f common.yaml
2. kubectl create -f operator.yaml
3. kubectl create -f cluster-test.yaml
</code></pre></div><p><a href=https://rook.io/docs/rook/v1.2/ceph-quickstart.html>https://rook.io/docs/rook/v1.2/ceph-quickstart.html</a></p><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=9 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=rook-cephクラスター構築external-cluster>Rook-Cephクラスター構築（external cluster）</h6><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2</span>
git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
cd cluster/examples/kubernetes/ceph

1. kubectl create -f common.yaml
2. kubectl create -f operator.yaml
3. kubectl create -f common-external.yaml
4. ConfigMap/Secretリソースに外部のCeph clusterの情報としていれる
  - namespace
  - FSID
  - client admin
  - monitor endpoint
5. kubectl create -f cluster-external.yaml
</code></pre></div><p>クラスター構築後は</p><ul><li>local: OSD / MON / MGRは存在しない</li><li>External: Stateが、Createdではなく、Connectedの状態になる</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=12 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=2-rook-ceph-external-clusterの利用方法>2. Rook-Ceph External Clusterの利用方法</h5><hr><ul><li>ストレージリソースを用意する際、local cluster側の操作のみで完結できるのはよき。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=3-rook-ceph-external-cluster利用時の注意点>3. Rook-Ceph External Cluster利用時の注意点</h5><hr><ul><li>1番気をつけた方がいいのは、Cephバージョンかな？</li><li>configmapの修正が必要なのは、嫌だね。（いずれに、新しく最新versionを構築する場合は関係なさそう。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=14 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=参考リンク>参考リンク</h5><ul><li><a href=https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#external-cluster>Rook Doc - Ceph Cluster CRD #External Cluster</a></li><li><a href=https://github.com/rook/rook/blob/master/design/ceph/ceph-external-cluster.md>GitHub - Rook and External Ceph Clusters</a></li><li><a href=https://github.com/rook/rook/issues/4816>GitHub Issues - External cluster details are not populated to configmap rook-ceph-csi-config</a></li></ul><h3 id=rook-cephでいろいろベンチマークとってみる-japan_rookhttpstwittercomjapan_rook>Rook-Cephでいろいろベンチマークとってみる <a href=https://twitter.com/japan_rook>@japan_rook</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=rook-cephでio計測をする>Rook CephでIO計測をする</h4><h5 id=モチベーション>モチベーション</h5><ul><li>IO測るの楽しい！笑</li><li>ストレージ直とRook-Cephを挟むとどれくらい変わるか</li><li>構成変更によるIOの変化</li></ul><h5 id=環境>環境</h5><ul><li>workerは、Rook-CephとIOをかけるが同居するため、割と強めにしている。</li><li>一応、現時点で最新versionの組み合わせ。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=5 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=遊び方>遊び方</h5><ol><li>FIO 3.13</li></ol><ul><li><a href=https://kubestone.io/en/latest/benchmarks/fio/>kubestone fio</a></li><li>fioのCustome Resouceが便利</li><li>ここの<a href=https://kubestone.io/en/latest/quickstart/>Quick Start</a>通りにやればokってことかな？</li></ul><ol start=2><li>fioのpodから100GBをマウント</li><li>時間の都合で、4K random read, 4K random writeのみ</li></ol><h5 id=何を測るか>何を測るか</h5><ol><li>素のEBS（gp2）vs Rook-Ceph 3x replica RBD</li><li>CephクラスタのOSD数</li><li>レプリカ数（一般的に、3xのreplicasだが。）</li></ol><h6 id=1-素のebsgp2vs-rook-ceph-3x-replica-rbd>1. 素のEBS（gp2）vs Rook-Ceph 3x replica RBD</h6><ul><li>負荷が低い時のwriteは結構違う。EBSの方が速い</li><li>個人的には、それぞれOSDを3つつけていて、並列にreadができるので、1個のEBSからreadするのと、3個のEBSから読み込むよりも有利なので、Rook-Cephの方が役に立つ（read強い。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=10 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=2-cephクラスタのosd数>2. CephクラスタのOSD数</h6><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=3-レプリカ数>3. レプリカ数</h6><ul><li>clush ruleにしたがって、平均になるように、replicaされている</li><li>なので、利用されるEBSの数は変わらない。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=16 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=まとめ>まとめ</h4><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=17 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=質疑応答-2>質疑応答</h4><blockquote><p>Q. MIN_SIZE（Ceph poolの最小サイズ？）は同じにしているのかな？</p><p>A. 同じ一にしています。2xでも死んでも計測し続けられるようにしています。</p></blockquote><blockquote><p>Q. best practiceで、1 SSD 1 device あたり 〇〇OSDに良いみたいな指針をみた気がするのですが。そういう指針に沿うと良いとか有りますか？</p><p>A. 有効に使う 1 SSD に対して、1 OSDにしても、あまりIOがこなければもったいないので、2とか4OSDにした方が良いという話もあります。感覚としては、一旦、1 SSDに対して、1~2 OSDが良いかなと。NVMeみたいに、並列にQueueがバシバシ入るものなら、4,6OSDでも良いかなと。もう、その辺はどの程度の負荷になるか。容量にするか。によって、変えてもらえればと思います。</p></blockquote><h2 id=さいごに>さいごに</h2><p>オンラインイベント初だったが、発表の音声ログなど残っていれば、そんなに問題ないなと思ったりしました。</p><div class=related><h3>Similar articles:</h3><ul><li><a href=/post/202002051855/>Terraform meetup tokyo#4</a></li><li><a href=/post/202001291937/>Kubernetes Meetup Tokyo #27</a></li><li><a href=/post/202001231917/>DMM.go #1</a></li><li><a href=/post/202001191310/>iOS・SwiftプロジェクトのCI/CDで、bitrise.ymlをリポジトリ内で管理する</a></li><li><a href=/post/202001150046/>Goもくもく会（ごもくかい）#23</a></li></ul></div></div></div></section><script src=/js/copycode.js></script><section class=section><div class="container has-text-centered"><p>(C) <a href=https://github.com/jkkitakita>Jun Kitamura</a> 2020</p><p>Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/ribice/kiss>Kiss</a>.</p></div></section><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-156067080-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript>var _paq=_paq||[];_paq.push(['trackPageView']);_paq.push(['enableLinkTracking']);(function(){var u="\/\/matomo.example.com\/";_paq.push(['setTrackerUrl',u+'piwik.php']);_paq.push(['setSiteId','1']);var d=document,g=d.createElement('script'),s=d.getElementsByTagName('script')[0];g.type='text/javascript';g.async=true;g.defer=true;g.src=u+'piwik.js';s.parentNode.insertBefore(g,s);})();</script><noscript><img src="//matomo.example.com/piwik.php?idsite=1&rec=1" style=border:0 alt></noscript></body></html>