<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://blog.jkkitakita.dev/favicon.ico><link rel=stylesheet href=/css/style.min.css><title>Japan Rook Meetup #2</title></head><body><header id=banner><h2><a href=https://blog.jkkitakita.dev>jkkitakita</a></h2><nav><ul><li><a href=/ title=posts>posts</a></li><li><a href=/about/ title=about>about</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Japan Rook Meetup #2</h1><div><time>March 29, 2020</time></div></header><h2 id=はじめに>はじめに</h2><p>今回はリモートでの開催だった<a href=https://rook.connpass.com/event/160657/>Japan Rook Meetup #2</a>に関するブログ枠です。基本的に、意見ほぼなしのまとめです。GlusterFS とかはなんとなく触ったことがあるのですが、Rook とか Ceph とかはほぼ触ったことがなかったので、新鮮に勉強させていただきました。何かここが違うなど問題があれば、twitter 等でご指摘いただければと思います。</p><h2 id=アジェンダ>アジェンダ</h2><p><img src=/images/agenda_rook_meetup_2.png alt=agenda_rook_meetup_2></p><h2 id=ハッシュタグ>ハッシュタグ</h2><p><a href="https://twitter.com/search?q=%23k8sjp">#k8sjp</a></p><h2 id=youtube>Youtube</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/xSLIrfkBKv4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=内容>内容</h2><h3 id=rook-基礎バージョンアップ-rev4thttpstwittercomrev4t>Rook 基礎・バージョンアップ <a href=https://twitter.com/rev4t>@rev4t</a></h3><hr><iframe src=//www.slideshare.net/slideshow/embed_code/key/8Fiu2FQopxr41n width=595 height=376 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe><h4 id=そもそもどういう時に利用する物なのか>そもそもどういう時に利用する物なのか</h4><p>Kubernetes で外部ストレージを利用したい時</p><h4 id=解決できること>解決できること</h4><p>k8s 上で Storage Operation を実現。storage は運用が大変だが、その自動化ができる。</p><h4 id=rook-とは>Rook とは</h4><p>CNCF。Operator + Custom Resouce で k8s 拡張可能。<br>また、沢山の Storage システムに対応したフレームワーク</p><ul><li>Ceph（Stable）</li><li>EdgeFS（Stable）</li><li>CockroachDB</li><li>Cassandra</li><li>NFS</li><li>YugabyteDB</li><li>minio</li></ul><h4 id=構成要素>構成要素</h4><ul><li>Rook Operator<br>実際にストレージのデプロイ・管理をしてくれるもの</li><li>Rook Discover<br>ストレージノードの変更等を検知して、Operator に伝える etc&mldr;</li></ul><h4 id=version-up>Version Up</h4><p>● Rook（major version）</p><ol><li>前提として、Ceph が正常に起動していることを確認</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># cephのステータス確認</span>
</span></span><span class=line><span class=cl>ceph status
</span></span></code></pre></div><ol start=2><li>Rook の Control plane が正常に稼働していることを確認</li><li>CRD と RBAC の更新</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g.</span>
</span></span><span class=line><span class=cl><span class=c1># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-apply.yaml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl apply -f upgrade-from-v1.1-apply.yaml
</span></span></code></pre></div><ol start=4><li>fuse or rdb-ndb を使っている場合、Cephfs plugin。rdb plugin の update strategy を onDelete に変更</li><li>Rook operator の update</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g. v1.2.5へ</span>
</span></span><span class=line><span class=cl>kubectl -n <span class=nv>$ROOK_SYSTEM_NAMESPACE</span> <span class=nb>set</span> image deploy/rook-ceph-oeprator rook-ceph-operator<span class=o>=</span>rook/ceph:v1.2.5
</span></span></code></pre></div><ol start=6><li>更新されるまで、しばらく待つ</li><li>fuse or rdb-ndb を使っている場合、CSI driver の pod を delete することで更新させる</li><li>最後に、CRD を更新</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g.</span>
</span></span><span class=line><span class=cl><span class=c1># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-crds.yaml</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kubectl apply -f upgrade-from-v1.1-crds.yaml
</span></span></code></pre></div><p>● Rook（minor version）</p><ol><li>image の version を更新するだけ。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g. v1.2.5へ</span>
</span></span><span class=line><span class=cl>kubectl -n <span class=nv>$ROOK_SYSTEM_NAMESPACE</span> <span class=nb>set</span> image deploy/rook-ceph-oeprator rook-ceph-operator<span class=o>=</span>rook/ceph:v1.2.5
</span></span></code></pre></div><p>（注）Ceph の delete/create が伴う場合があるそう。</p><p><img src="https://image.slidesharecdn.com/rook-meetup-2-sogabe-rev1-200327104438/95/rook-31-638.jpg?cb=1585306044" alt=rook-minor-upgrade></p><p>● Ceph（v14.2.4 -> v14.2.7）</p><ol><li>image の version を更新する。（image を更新すれば、あとは Rook Operator がやってくれる。）</li><li>完了するまで待つ。</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># cephのステータス確認して、HEALTH_OKかどうか</span>
</span></span><span class=line><span class=cl>ceph status
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cluster: id: 3casefd74-edfg-2352-b54j-51jk12tj10
</span></span><span class=line><span class=cl>health: HEALTH_OK
</span></span></code></pre></div><p>● External Cluster</p><p>既存の Ceph Cluster に対して、Rook を用いた k8s との連携ができる。</p><p>■ メリット</p><ol><li>CSI Driver の運用管理のみを k8s で実施する形になり、k8s 側の運用コストを下げつつ、責任分界点を分けられる</li><li>Rook Version Up の際に、Ceph Cluster に与えるサービス影響を完全に排除できる</li><li>k8s networking による性能低下の影響をうけなくなる。</li></ol><p>■ デメリット</p><ol><li>Rook の他に、Ceph のデプロイツールの導入をする必要が出てくる</li></ol><p>■ 利用例</p><p>Rook + ceph-ansible<br><a href=https://github.com/ceph/ceph-ansible>https://github.com/ceph/ceph-ansible</a></p><h4 id=summary>Summary</h4><ul><li>長期的な運用で、Storage Backend だけでなく、Rook Operaotr 自体の運用もしなくてはならない。</li><li>Ceph の version がすごく楽になる。</li><li>k8s 側の運用をシンプルにしたい場合は、ceph-ansible + Rook external cluster が良さそう。</li></ul><h4 id=質疑応答>質疑応答</h4><blockquote><p>Q. Ceph OSD の update で、node を変更しない以外のパターン以外で、新規の node に migration をかけることは可能か？<br>A. 検証していないが、多分できると思っている。</p></blockquote><blockquote><p>Q. Control Plan の upgrade する時に、実際にデータアクセスに影響があるということでしょうか？<br>A. Ceph OSD の pod を再起動されることになるので、クライアントからのリクエストの latency が高くなるだろうと思っている。影響の少ない時間帯で実施するなどの対応が必要だろうと思っている。</p></blockquote><h3 id=rookceph-upstream-最新状況-satoru_takeuchihttpstwittercomsatoru_takeuchi>Rook/Ceph upstream 最新状況 <a href=https://twitter.com/satoru_takeuchi>@satoru_takeuchi</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=目次>目次</h4><ol><li>前提知識: OSD 作成方法</li><li>Rook 最新安定版の新機能</li><li>Rook 次期安定版の開発状況</li><li>Rook を取り巻く状況</li></ol><h4 id=1-前提知識-osd-作成方法>1. 前提知識: OSD 作成方法</h4><hr><p>OSD の作成方法には大きく２つの方法があるとのこと</p><p>● 1.1. 従来の方法</p><p>OSD の設定にノード上のデバイスを直接指定する</p><ul><li>これだと、Rook/Ceph cluster 管理者にハードウェア構成の知識が必要</li><li>さらに、デバイス名直接指定しているので、自前で cluster を構築する必要がある</li></ul><p>● 1.2. OSD on PVC v1.1~</p><p>OSD の設定に PVC template を指定する。それを受けて、CSI driver が PV を作成する。Cluster 管理者は OSD pod がどこにあるかを気にしなくて良いさらに、デバイスの管理もしなくて良くなる。</p><h4 id=2-rook-最新安定版の新機能>2. Rook 最新安定版の新機能</h4><hr><p>● 2.1. OSD on PVC で LV サポート</p><ul><li>OSD on PVC でロジカルボリューム（LV）対応した（サイボウズ作）</li></ul><p>■ 目的・背景</p><ol><li>ローカルの NVMe SSD 上に OSD を作成し、OSD を束ねて、Ceph Cluster を構築し、ブロックデバイス（RDB？）を提供したい</li><li>管理コスト削減のため、PV を dynamic provisioning したい。pv が欲しくなったら、dynamic な storage が欲しい。</li></ol><p>■ 既存の課題</p><p>以下の条件を満たす CSI ドライバがない</p><ul><li>ローカルボリューム</li><li>dynamic provisioning</li><li>実用レベルのものがない</li></ul><p>つまり、デバイス追加は手作業で PV 作成する必要がある。</p><p>■ TopoLVM</p><ul><li>PVC の変更を検知して、PV を dynamiv provisioning してくれるやつ？</li></ul><p>● 2.2. 従来方式で udev persistent name をサポート</p><p>v1.1 以前では、/dev 直下は気付いたら変わりうる。e.g. sda -> sdb<br>v1.2 以降では、新機能 devicePathFilter（サイボウズ作）</p><ul><li>こうすると、/dev 直下の device name は変わりうるのだが、by-path 名は変更されない</li><li>そのため、データ破壊を防げる</li></ul><p>● 2.3. その他変更</p><ol><li><p>cech-crash collector</p><ul><li>daemon のクラッシュ情報を Ceph クラスタに保存できるので、トラブルシューティングに効果的</li><li>デフォルトで有効</li></ul></li><li><p>FileStore OSD が obsolete（既存の FileStore OSD はサポートし続ける）</p></li></ol><h4 id=3-rook-次期安定版の開発状況>3. Rook 次期安定版の開発状況</h4><hr><p>● 3.1. Failure domain をまたいだ OSD の均等分散配置</p><p>k8s の TopologySpreadConstraints 機能のサポート（OSD on PVC に必須）</p><p>■ TopologySpreadConstraints の目的</p><ol><li>ラック障害耐性</li><li>ノード障害耐性</li></ol><p>■ これまでの課題</p><p>OSD pod の偏りが生まれてしまい、ラックの障害耐性がない場合が有り得る。次期安定版から、対応予定で、これでようやく OSD on PVC が使えるものとなると思っている。</p><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide=20 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>● 3.2. OSD on LV-backed PVC のテスト追加</p><ol><li>Rook は PR マージ時、テスト全パスが必須。</li></ol><p>しかし、3 つの壁が。。。</p><p>■ 壁 1: master で OSD on LV-backed PVC が動かない</p><ol><li>この PR でリグレッションが発生
<a href=https://github.com/rook/rook/pull/4435>https://github.com/rook/rook/pull/4435</a></li><li>Issue 発行
<a href=https://github.com/rook/rook/issues/5075>https://github.com/rook/rook/issues/5075</a></li><li>CI 重要！！！！</li></ol><p>■ 壁 2: テストがテスト環境を破壊</p><ol><li>テスト冒頭で全 LV/VG/PV を強制削除</li><li>PR マージ待ち
<a href=https://github.com/rook/rook/pull/4966>https://github.com/rook/rook/pull/4966</a></li></ol><p>■ 壁 3: ローカル環境でテストがパスしない</p><p>調査中</p><p>● 3.3. FileStore の新規作成が不可能に</p><p>obsolute ではなく、不可能へ。</p><p>● 3.4. ストレージプロバイダ</p><ol><li>新しいストレージプロバイダなし</li><li>minio のコードが削除（メンテされていないから。）</li></ol><p>● 3.5. その他の課題</p><p>コア部分と前ストレージプロバイダが同じリポジトリ管理しているので、開発がしにくい</p><ul><li>kubernetes と storage plugin を別リポジトリを分けた話と同じ感じ。</li></ul><p>● 3.6. 今後の貢献予定</p><ol><li>テストの充実化</li><li>暗号化デバイス上の OSD</li><li>バグ解決</li></ol><h4 id=4-rook-を取り巻く状況>4. Rook を取り巻く状況</h4><hr><p>● 4.1. プロジェクトの成熟度</p><ul><li>現在は、Incubating</li><li>もうちょっとで、卒業するかも？
<a href=https://github.com/cncf/toc/pull/366>https://github.com/cncf/toc/pull/366</a></li></ul><p>● 4.2. Rook/Ceph を使う製品</p><ul><li><a href=https://blog.openshift.com/introducing-openshift-container-storage-4-2/>Red Hat OpenShift Container Storage4（GA・2020-01-15）</a></li><li><a href=https://www.suse.com/c/ceph-on-kubernetes-tech-preview/>Containerized SUSE Enterprise Storage on CaaSP（Tecknical preview）</a></li></ul><h4 id=5-参考リンク>5. 参考リンク</h4><ul><li><a href=https://blog.cybozu.io/entry/2019/12/03/114746>ストレージオーケストレーター Rook へのサイボウズのコミット方針</a></li><li><a href=https://blog.cybozu.io/entry/2019/11/08/090000>Kubernetes でローカルストレージを有効活用しよう</a></li><li><a href="https://docs.google.com/presentation/d/1mMPYMDC4JMGWhoL3FzFgeasSLJepNwYMfwQD-T_gET4/edit#slide=id.g3a79217937_0_104">Rook - Graduation Proposal</a></li><li>賢く「散らす」ための Topology Spread Constraints<div class=embed><script async class=speakerdeck-embed data-id=61215f14bf534bce8ee5726a2ce243dd data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div></li></ul><h4 id=質疑応答-1>質疑応答</h4><blockquote><p>Q. TopoLVM に関して、LVM のレイヤーを declaretive なやり方で、VG？を作ってくれるものという認識であっていますでしょうか？</p><p>A. はい。その通りです。Storage Class で TopoLVM のものを指定していただければ、LV を勝手に切ってくれて、それをブロックデバイス、ないし、ファイルシステムとして利用できるものです。</p></blockquote><blockquote><p>Q. FileStore が obsolute になって、BlueStore への移行がこれから出てくると思うのだが、マイグレーションパスの開発は進んでいるのでしょうか？</p><p>A. 現在、ドキュメント整備中です。基本的には、そのドキュメントがみてやっていただければ良いようになっていく予定です。</p></blockquote><h3 id=rook-ceph-で-external-cluster-を利用する-futa_0203httpstwittercomfuta_0203>Rook-Ceph で External Cluster を利用する <a href=https://twitter.com/FUTA_0203>@FUTA_0203</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=本日お話しすること>本日お話しすること</h4><ol><li>Rook-Ceph External Cluster の概要</li><li>Rook-Ceph External Cluster の利用方法</li><li>Rook-Ceph External Cluster 利用時の注意点</li></ol><p>● 1. Rook-Ceph External Cluster の概要</p><hr><p>Rook-Ceph を構築したクラスター外に存在する、Ceph クラスターのストレージリソースを利用すること</p><ul><li>local: k8s（Rook） cluster</li><li>external: Ceph cluster</li></ul><p>■ 本機能の背景</p><ol><li>Rook ver 1.1 から利用可能</li><li>基本的には、k8s 内の storage を利用する想定だった</li><li>しかし、そうではない、ユースケースもあったため。</li></ol><p>そうではないユースケースとは&mldr;</p><ul><li>既存 Ceph Cluster が存在する</li><li>1 つの Ceph クラスターのリソースを複数 k8s で利用したい</li><li>単純に storage を分離したい</li></ul><p>■ Rook-Ceph クラスター構築（通常）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g. v1.2</span>
</span></span><span class=line><span class=cl>git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> cluster/examples/kubernetes/ceph
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1. kubectl create -f common.yaml
</span></span><span class=line><span class=cl>2. kubectl create -f operator.yaml
</span></span><span class=line><span class=cl>3. kubectl create -f cluster-test.yaml
</span></span></code></pre></div><p><a href=https://rook.io/docs/rook/v1.2/ceph-quickstart.html>https://rook.io/docs/rook/v1.2/ceph-quickstart.html</a></p><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=9 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>■ Rook-Ceph クラスター構築（external cluster）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># e.g. v1.2</span>
</span></span><span class=line><span class=cl>git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> cluster/examples/kubernetes/ceph
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>1. kubectl create -f common.yaml
</span></span><span class=line><span class=cl>2. kubectl create -f operator.yaml
</span></span><span class=line><span class=cl>3. kubectl create -f common-external.yaml
</span></span><span class=line><span class=cl>4. ConfigMap/Secretリソースに外部のCeph clusterの情報としていれる
</span></span><span class=line><span class=cl>  - namespace
</span></span><span class=line><span class=cl>  - FSID
</span></span><span class=line><span class=cl>  - client admin
</span></span><span class=line><span class=cl>  - monitor endpoint
</span></span><span class=line><span class=cl>5. kubectl create -f cluster-external.yaml
</span></span></code></pre></div><p>クラスター構築後は</p><ul><li>local: OSD / MON / MGR は存在しない</li><li>External: State が、Created ではなく、Connected の状態になる</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=12 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>● 2. Rook-Ceph External Cluster の利用方法</p><hr><ul><li>ストレージリソースを用意する際、local cluster 側の操作のみで完結できるのはよき。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>● 3. Rook-Ceph External Cluster 利用時の注意点</p><hr><ul><li>1 番気をつけた方がいいのは、Ceph バージョンかな？</li><li>configmap の修正が必要なのは、嫌だね。（いずれに、新しく最新 version を構築する場合は関係なさそう。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=14 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>● 参考リンク</p><ul><li><a href=https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#external-cluster>Rook Doc - Ceph Cluster CRD #External Cluster</a></li><li><a href=https://github.com/rook/rook/blob/master/design/ceph/ceph-external-cluster.md>GitHub - Rook and External Ceph Clusters</a></li><li><a href=https://github.com/rook/rook/issues/4816>GitHub Issues - External cluster details are not populated to configmap rook-ceph-csi-config</a></li></ul><h3 id=rook-ceph-でいろいろベンチマークとってみる-japan_rookhttpstwittercomjapan_rook>Rook-Ceph でいろいろベンチマークとってみる <a href=https://twitter.com/japan_rook>@japan_rook</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=rook-ceph-で-io-計測をする>Rook Ceph で IO 計測をする</h4><p>● モチベーション</p><ul><li>IO 測るの楽しい！笑</li><li>ストレージ直と Rook-Ceph を挟むとどれくらい変わるか</li><li>構成変更による IO の変化</li></ul><p>● 環境</p><ul><li>worker は、Rook-Ceph と IO をかけるが同居するため、割と強めにしている。</li><li>一応、現時点で最新 version の組み合わせ。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=5 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>● 遊び方</p><ol><li>FIO 3.13</li></ol><ul><li><a href=https://kubestone.io/en/latest/benchmarks/fio/>kubestone fio</a></li><li>fio の Custom Resource が便利</li><li>ここの<a href=https://kubestone.io/en/latest/quickstart/>Quick Start</a>通りにやれば ok ってことかな？</li></ul><ol start=2><li>fio の pod から 100GB をマウント</li><li>時間の都合で、4K random read, 4K random write のみ</li></ol><p>● 何を測るか</p><ol><li>素の EBS（gp2）vs Rook-Ceph 3x replica RBD</li><li>Ceph クラスタの OSD 数</li><li>レプリカ数（一般的に、3x の replicas だが。）</li></ol><p>■ 1. 素の EBS（gp2）vs Rook-Ceph 3x replica RBD</p><ul><li>負荷が低い時の write は結構違う。EBS の方が速い</li><li>個人的には、それぞれ OSD を 3 つつけていて、並列に read ができるので、1 個の EBS から read するのと、3 個の EBS から読み込むよりも有利なので、Rook-Ceph の方が役に立つ（read 強い。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=10 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>■ 2. Ceph クラスタの OSD 数</p><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><p>■ 3. レプリカ数</p><ul><li>clush rule にしたがって、平均になるように、replica されている</li><li>なので、利用される EBS の数は変わらない。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=16 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=まとめ>まとめ</h4><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=17 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=質疑応答-2>質疑応答</h4><blockquote><p>Q. MIN_SIZE（Ceph pool の最小サイズ？）は同じにしているのかな？</p><p>A. 同じ一にしています。2x でも死んでも計測し続けられるようにしています。</p></blockquote><blockquote><p>Q. best practice で、1 SSD 1 device あたり 〇〇 OSD に良いみたいな指針をみた気がするのですが。そういう指針に沿うと良いとか有りますか？</p><p>A. 有効に使う 1 SSD に対して、1 OSD にしても、あまり IO がこなければもったいないので、2 とか 4OSD にした方が良いという話もあります。感覚としては、一旦、1 SSD に対して、1~2 OSD が良いかなと。NVMe みたいに、並列に Queue がバシバシ入るものなら、4,6OSD でも良いかなと。もう、その辺はどの程度の負荷になるか。容量にするか。によって、変えてもらえればと思います。</p></blockquote><h2 id=さいごに>さいごに</h2><p>オンラインイベント初だったが、発表の音声ログなど残っていれば、そんなに問題ないなと思ったりしました。</p></article></main><footer id=footer>Copyright © 2020 Jun Kitamura</footer></body></html>