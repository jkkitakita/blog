<!doctype html><html xmlns=http://www.w3.org/1999/xhtml lang=ja><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Japan Rook Meetup #2 | jkkitakita</title><meta property="og:title" content="Japan Rook Meetup #2 - jkkitakita"><meta property="og:description" content="Japan Rook Meetup #2"><meta property="og:url" content="https://blog.jkkitakita.dev/post/202003291728/"><meta property="og:site_name" content="jkkitakita"><meta property="og:type" content="article"><meta property="og:image" content="https://www.gravatar.com/avatar/23c25c7545ad2fdbdbab1f9201bf96b7?s=256"><meta property="article:author" content="https://facebook.com/jkkitakita"><meta property="article:section" content="Post"><meta property="article:tag" content="rook"><meta property="article:tag" content="event"><meta property="article:published_time" content="2020-03-29T17:28:37+09:00"><meta property="article:modified_time" content="2020-03-29T17:28:37+09:00"><meta name=twitter:card content="summary"><meta name=twitter:site content="@jkkitakita"><meta name=twitter:creator content="@jkkitakita"><link href=https://blog.jkkitakita.dev/index.xml rel=alternate type=application/rss+xml title=jkkitakita><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=https://blog.jkkitakita.dev/css/custom.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://blog.jkkitakita.dev/post/202003291728/><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"></head><body><section class=section><div class=container><nav id=nav-main class=nav><div id=nav-name class=nav-left><a id=nav-anchor class=nav-item href=https://blog.jkkitakita.dev><h1 id=nav-heading class="title is-4">jkkitakita</h1></a></div><div class=nav-right><nav id=nav-items class="nav-item level is-mobile"><a class=level-item aria-label=github href=https://github.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77a5.44 5.44.0 00-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></i></span></a><a class=level-item aria-label=facebook href=https://facebook.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M18 2h-3a5 5 0 00-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 011-1h3z"/></svg></i></span></a><a class=level-item aria-label=twitter href=https://twitter.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg></i></span></a><a class=level-item aria-label=instagram href=https://instagram.com/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><rect x="2" y="2" width="20" height="20" rx="5" ry="5"/><path d="M16 11.37A4 4 0 1112.63 8 4 4 0 0116 11.37z"/><line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/></svg></i></span></a><a class=level-item aria-label=email href=mailto:jkkitakita@gmail.com target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></i></span></a><a class=level-item aria-label=linkedin href=https://linkedin.com/in/jkkitakita target=_blank rel=noopener><span class=icon><i><svg viewbox="0 0 24 24" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" aria-hidden="true"><path stroke-width="1.8" d="m5.839218 4.101561c0 1.211972-.974141 2.194011-2.176459 2.194011S1.4863 5.313533 1.4863 4.101561c0-1.211094.974141-2.194011 2.176459-2.194011s2.176459.982917 2.176459 2.194011zm.017552 3.94922h-4.388022v14.04167H5.85677V8.050781zm7.005038.0H8.501869v14.04167h4.360816v-7.370999c0-4.098413 5.291077-4.433657 5.291077.0v7.370999h4.377491v-8.89101c0-6.915523-7.829986-6.66365-9.669445-3.259423V8.050781z"/></svg></i></span></a></nav></div></nav><nav class=nav></nav></div><script src=/js/navicon-shift.js></script></section><section class=section><div class=container><div class="subtitle tags is-6 is-pulled-right"><a class="subtitle is-6" href=/tags/rook/>#rook</a>
| <a class="subtitle is-6" href=/tags/event/>#event</a></div><h2 class="subtitle is-6">March 29, 2020</h2><h1 class=title>Japan Rook Meetup #2</h1><div class=content><h2 id=はじめに>はじめに</h2><p>今回はリモートでの開催だった<a href=https://rook.connpass.com/event/160657/>Japan Rook Meetup #2</a>に関するブログ枠です。基本的に、意見ほぼなしのまとめです。GlusterFS とかはなんとなく触ったことがあるのですが、Rook とか Ceph とかはほぼ触ったことがなかったので、新鮮に勉強させていただきました。何かここが違うなど問題があれば、twitter 等でご指摘いただければと思います。</p><h2 id=アジェンダ>アジェンダ</h2><p><img src=/images/agenda_rook_meetup_2.png alt=agenda_rook_meetup_2></p><h2 id=ハッシュタグ>ハッシュタグ</h2><p><a href="https://twitter.com/search?q=%23k8sjp">#k8sjp</a></p><h2 id=youtube>Youtube</h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/xSLIrfkBKv4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h2 id=内容>内容</h2><h3 id=rook-基礎バージョンアップ-rev4thttpstwittercomrev4t>Rook 基礎・バージョンアップ <a href=https://twitter.com/rev4t>@rev4t</a></h3><hr><iframe src=//www.slideshare.net/slideshow/embed_code/key/8Fiu2FQopxr41n width=595 height=376 frameborder=0 marginwidth=0 marginheight=0 scrolling=no style="border:1px solid #ccc;border-width:1px;margin-bottom:5px;max-width:100%" allowfullscreen></iframe><h4 id=そもそもどういう時に利用する物なのか>そもそもどういう時に利用する物なのか</h4><p>Kubernetes で外部ストレージを利用したい時</p><h4 id=解決できること>解決できること</h4><p>k8s 上で Storage Operation を実現。storage は運用が大変だが、その自動化ができる。</p><h4 id=rook-とは>Rook とは</h4><p>CNCF。Operator + Custom Resouce で k8s 拡張可能。<br>また、沢山の Storage システムに対応したフレームワーク</p><ul><li>Ceph（Stable）</li><li>EdgeFS（Stable）</li><li>CockroachDB</li><li>Cassandra</li><li>NFS</li><li>YugabyteDB</li><li>minio</li></ul><h4 id=構成要素>構成要素</h4><ul><li>Rook Operator<br>実際にストレージのデプロイ・管理をしてくれるもの</li><li>Rook Discover<br>ストレージノードの変更等を検知して、Operator に伝える etc&mldr;</li></ul><h4 id=version-up>Version Up</h4><h5 id=rookmajor-version>Rook（major version）</h5><ol><li>前提として、Ceph が正常に起動していることを確認</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># cephのステータス確認</span>
ceph status
</code></pre></div><ol start=2><li>Rook の Control plane が正常に稼働していることを確認</li><li>CRD と RBAC の更新</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g.</span>
<span style=color:#75715e># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-apply.yaml</span>

kubectl apply -f upgrade-from-v1.1-apply.yaml
</code></pre></div><ol start=4><li>fuse or rdb-ndb を使っている場合、Cephfs plugin。rdb plugin の update strategy を onDelete に変更</li><li>Rook operator の update</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2.5へ</span>
kubectl -n $ROOK_SYSTEM_NAMESPACE set image deploy/rook-ceph-oeprator rook-ceph-operator<span style=color:#f92672>=</span>rook/ceph:v1.2.5
</code></pre></div><ol start=6><li>更新されるまで、しばらく待つ</li><li>fuse or rdb-ndb を使っている場合、CSI driver の pod を delete することで更新させる</li><li>最後に、CRD を更新</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g.</span>
<span style=color:#75715e># https://github.com/rook/rook/blob/v1.2.0/cluster/examples/kubernetes/ceph/upgrade-from-v1.1-crds.yaml</span>

kubectl apply -f upgrade-from-v1.1-crds.yaml
</code></pre></div><h5 id=rookminor-version>Rook（minor version）</h5><ol><li>image の version を更新するだけ。</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2.5へ</span>
kubectl -n $ROOK_SYSTEM_NAMESPACE set image deploy/rook-ceph-oeprator rook-ceph-operator<span style=color:#f92672>=</span>rook/ceph:v1.2.5
</code></pre></div><p>（注）Ceph の delete/create が伴う場合があるそう。</p><p><img src="https://image.slidesharecdn.com/rook-meetup-2-sogabe-rev1-200327104438/95/rook-31-638.jpg?cb=1585306044" alt=rook-minor-upgrade></p><h5 id=cephv1424---v1427>Ceph（v14.2.4 -> v14.2.7）</h5><ol><li>image の version を更新する。（image を更新すれば、あとは Rook Operator がやってくれる。）</li><li>完了するまで待つ。</li></ol><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># cephのステータス確認して、HEALTH_OKかどうか</span>
ceph status

cluster: id: 3casefd74-edfg-2352-b54j-51jk12tj10
health: HEALTH_OK
</code></pre></div><h5 id=external-cluster>External Cluster</h5><p>既存の Ceph Cluster に対して、Rook を用いた k8s との連携ができる。</p><h6 id=メリット>メリット</h6><ol><li>CSI Driver の運用管理のみを k8s で実施する形になり、k8s 側の運用コストを下げつつ、責任分界点を分けられる</li><li>Rook Version Up の際に、Ceph Cluster に与えるサービス影響を完全に排除できる</li><li>k8s networking による性能低下の影響をうけなくなる。</li></ol><h6 id=デメリット>デメリット</h6><ol><li>Rook の他に、Ceph のデプロイツールの導入をする必要が出てくる</li></ol><h6 id=利用例>利用例</h6><p>Rook + ceph-ansible<br><a href=https://github.com/ceph/ceph-ansible>https://github.com/ceph/ceph-ansible</a></p><h4 id=summary>Summary</h4><ul><li>長期的な運用で、Storage Backend だけでなく、Rook Operaotr 自体の運用もしなくてはならない。</li><li>Ceph の version がすごく楽になる。</li><li>k8s 側の運用をシンプルにしたい場合は、ceph-ansible + Rook external cluster が良さそう。</li></ul><h4 id=質疑応答>質疑応答</h4><blockquote><p>Q. Ceph OSD の update で、node を変更しない以外のパターン以外で、新規の node に migration をかけることは可能か？<br>A. 検証していないが、多分できると思っている。</p></blockquote><blockquote><p>Q. Control Plan の upgrade する時に、実際にデータアクセスに影響があるということでしょうか？<br>A. Ceph OSD の pod を再起動されることになるので、クライアントからのリクエストの latency が高くなるだろうと思っている。影響の少ない時間帯で実施するなどの対応が必要だろうと思っている。</p></blockquote><h3 id=rookceph-upstream-最新状況-satoru_takeuchihttpstwittercomsatoru_takeuchi>Rook/Ceph upstream 最新状況 <a href=https://twitter.com/satoru_takeuchi>@satoru_takeuchi</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=目次>目次</h4><ol><li>前提知識: OSD 作成方法</li><li>Rook 最新安定版の新機能</li><li>Rook 次期安定版の開発状況</li><li>Rook を取り巻く状況</li></ol><h4 id=1-前提知識-osd-作成方法>1. 前提知識: OSD 作成方法</h4><hr><p>OSD の作成方法には大きく２つの方法があるとのこと</p><h5 id=11-従来の方法>1.1. 従来の方法</h5><p>OSD の設定にノード上のデバイスを直接指定する</p><ul><li>これだと、Rook/Ceph cluster 管理者にハードウェア構成の知識が必要</li><li>さらに、デバイス名直接指定しているので、自前で cluster を構築する必要がある</li></ul><h5 id=12-osd-on-pvc-v11>1.2. OSD on PVC v1.1~</h5><p>OSD の設定に PVC template を指定する。それを受けて、CSI driver が PV を作成する。Cluster 管理者は OSD pod がどこにあるかを気にしなくて良いさらに、デバイスの管理もしなくて良くなる。</p><h4 id=2-rook-最新安定版の新機能>2. Rook 最新安定版の新機能</h4><hr><h5 id=21-osd-on-pvc-で-lv-サポート>2.1. OSD on PVC で LV サポート</h5><ul><li>OSD on PVC でロジカルボリューム（LV）対応した（サイボウズ作）</li></ul><h6 id=目的背景>目的・背景</h6><ol><li>ローカルの NVMe SSD 上に OSD を作成し、OSD を束ねて、Ceph Cluster を構築し、ブロックデバイス（RDB？）を提供したい</li><li>管理コスト削減のため、PV を dynamic provisioning したい。pv が欲しくなったら、dynamic な storage が欲しい。</li></ol><h6 id=既存の課題>既存の課題</h6><p>以下の条件を満たす CSI ドライバがない</p><ul><li>ローカルボリューム</li><li>dynamic provisioning</li><li>実用レベルのものがない</li></ul><p>つまり、デバイス追加は手作業で PV 作成する必要がある。</p><h6 id=topolvm>TopoLVM</h6><ul><li>PVC の変更を検知して、PV を dynamiv provisioning してくれるやつ？</li></ul><h5 id=22-従来方式で-udev-persistent-name-をサポート>2.2. 従来方式で udev persistent name をサポート</h5><p>v1.1 以前では、/dev 直下は気付いたら変わりうる。e.g. sda -> sdb<br>v1.2 以降では、新機能 devicePathFilter（サイボウズ作）</p><ul><li>こうすると、/dev 直下の device name は変わりうるのだが、by-path 名は変更されない</li><li>そのため、データ破壊を防げる</li></ul><h5 id=23-その他変更>2.3. その他変更</h5><ol><li>cech-crash collector</li></ol><ul><li>daemon のクラッシュ情報を Ceph クラスタに保存できるので、トラブルシューティングに効果的</li><li>デフォルトで有効</li></ul><ol start=2><li>FileStore OSD が obsolete（既存の FileStore OSD はサポートし続ける）</li></ol><h4 id=3-rook-次期安定版の開発状況>3. Rook 次期安定版の開発状況</h4><hr><h5 id=31-failure-domain-をまたいだ-osd-の均等分散配置>3.1. Failure domain をまたいだ OSD の均等分散配置</h5><p>k8s の TopologySpreadConstraints 機能のサポート（OSD on PVC に必須）</p><h6 id=topologyspreadconstraints-の目的>TopologySpreadConstraints の目的</h6><ol><li>ラック障害耐性</li><li>ノード障害耐性</li></ol><h6 id=これまでの課題>これまでの課題</h6><p>OSD pod の偏りが生まれてしまい、ラックの障害耐性がない場合が有り得る。次期安定版から、対応予定で、これでようやく OSD on PVC が使えるものとなると思っている。</p><div class=embed><script async class=speakerdeck-embed data-id=a2fc007ecacd49d882d04677392176a2 data-slide=20 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=32-osd-on-lv-backed-pvc-のテスト追加>3.2. OSD on LV-backed PVC のテスト追加</h5><ol><li>Rook は PR マージ時、テスト全パスが必須。</li></ol><p>しかし、3 つの壁が。。。</p><h6 id=壁-1-master-で-osd-on-lv-backed-pvc-が動かない>壁 1: master で OSD on LV-backed PVC が動かない</h6><ol><li>この PR でリグレッションが発生
<a href=https://github.com/rook/rook/pull/4435>https://github.com/rook/rook/pull/4435</a></li><li>Issue 発行
<a href=https://github.com/rook/rook/issues/5075>https://github.com/rook/rook/issues/5075</a></li><li>CI 重要！！！！</li></ol><h6 id=壁-2-テストがテスト環境を破壊>壁 2: テストがテスト環境を破壊</h6><ol><li>テスト冒頭で全 LV/VG/PV を強制削除</li><li>PR マージ待ち
<a href=https://github.com/rook/rook/pull/4966>https://github.com/rook/rook/pull/4966</a></li></ol><h6 id=壁-3-ローカル環境でテストがパスしない>壁 3: ローカル環境でテストがパスしない</h6><p>調査中</p><h5 id=33-filestore-の新規作成が不可能に>3.3. FileStore の新規作成が不可能に</h5><p>obsolute ではなく、不可能へ。</p><h5 id=34-ストレージプロバイダ>3.4. ストレージプロバイダ</h5><ol><li>新しいストレージプロバイダなし</li><li>minio のコードが削除（メンテされていないから。）</li></ol><h5 id=35-その他の課題>3.5. その他の課題</h5><p>コア部分と前ストレージプロバイダが同じリポジトリ管理しているので、開発がしにくい</p><ul><li>kubernetes と storage plugin を別リポジトリを分けた話と同じ感じ。</li></ul><h5 id=36-今後の貢献予定>3.6. 今後の貢献予定</h5><ol><li>テストの充実化</li><li>暗号化デバイス上の OSD</li><li>バグ解決</li></ol><h4 id=4-rook-を取り巻く状況>4. Rook を取り巻く状況</h4><hr><h5 id=41-プロジェクトの成熟度>4.1. プロジェクトの成熟度</h5><ul><li>現在は、Incubating</li><li>もうちょっとで、卒業するかも？
<a href=https://github.com/cncf/toc/pull/366>https://github.com/cncf/toc/pull/366</a></li></ul><h5 id=42-rookceph-を使う製品>4.2. Rook/Ceph を使う製品</h5><ul><li><a href=https://blog.openshift.com/introducing-openshift-container-storage-4-2/>Red Hat OpenShift Container Storage4（GA・2020-01-15）</a></li><li><a href=https://www.suse.com/c/ceph-on-kubernetes-tech-preview/>Containerized SUSE Enterprise Storage on CaaSP（Tecknical preview）</a></li></ul><h4 id=5-参考リンク>5. 参考リンク</h4><ul><li><a href=https://blog.cybozu.io/entry/2019/12/03/114746>ストレージオーケストレーター Rook へのサイボウズのコミット方針</a></li><li><a href=https://blog.cybozu.io/entry/2019/11/08/090000>Kubernetes でローカルストレージを有効活用しよう</a></li><li><a href="https://docs.google.com/presentation/d/1mMPYMDC4JMGWhoL3FzFgeasSLJepNwYMfwQD-T_gET4/edit#slide=id.g3a79217937_0_104">Rook - Graduation Proposal</a></li><li>賢く「散らす」ための Topology Spread Constraints<div class=embed><script async class=speakerdeck-embed data-id=61215f14bf534bce8ee5726a2ce243dd data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div></li></ul><h4 id=質疑応答-1>質疑応答</h4><blockquote><p>Q. TopoLVM に関して、LVM のレイヤーを declaretive なやり方で、VG？を作ってくれるものという認識であっていますでしょうか？</p><p>A. はい。その通りです。Storage Class で TopoLVM のものを指定していただければ、LV を勝手に切ってくれて、それをブロックデバイス、ないし、ファイルシステムとして利用できるものです。</p></blockquote><blockquote><p>Q. FileStore が obsolute になって、BlueStore への移行がこれから出てくると思うのだが、マイグレーションパスの開発は進んでいるのでしょうか？</p><p>A. 現在、ドキュメント整備中です。基本的には、そのドキュメントがみてやっていただければ良いようになっていく予定です。</p></blockquote><h3 id=rook-ceph-で-external-cluster-を利用する-futa_0203httpstwittercomfuta_0203>Rook-Ceph で External Cluster を利用する <a href=https://twitter.com/FUTA_0203>@FUTA_0203</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=本日お話しすること>本日お話しすること</h4><ol><li>Rook-Ceph External Cluster の概要</li><li>Rook-Ceph External Cluster の利用方法</li><li>Rook-Ceph External Cluster 利用時の注意点</li></ol><h5 id=1-rook-ceph-external-cluster-の概要>1. Rook-Ceph External Cluster の概要</h5><hr><p>Rook-Ceph を構築したクラスター外に存在する、Ceph クラスターのストレージリソースを利用すること</p><ul><li>local: k8s（Rook） cluster</li><li>external: Ceph cluster</li></ul><h6 id=本機能の背景>本機能の背景</h6><ol><li>Rook ver 1.1 から利用可能</li><li>基本的には、k8s 内の storage を利用する想定だった</li><li>しかし、そうではない、ユースケースもあったため。</li></ol><p>そうではないユースケースとは&mldr;</p><ul><li>既存 Ceph Cluster が存在する</li><li>1 つの Ceph クラスターのリソースを複数 k8s で利用したい</li><li>単純に storage を分離したい</li></ul><h6 id=rook-ceph-クラスター構築通常>Rook-Ceph クラスター構築（通常）</h6><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2</span>
git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
cd cluster/examples/kubernetes/ceph

1. kubectl create -f common.yaml
2. kubectl create -f operator.yaml
3. kubectl create -f cluster-test.yaml
</code></pre></div><p><a href=https://rook.io/docs/rook/v1.2/ceph-quickstart.html>https://rook.io/docs/rook/v1.2/ceph-quickstart.html</a></p><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=9 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=rook-ceph-クラスター構築external-cluster>Rook-Ceph クラスター構築（external cluster）</h6><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># e.g. v1.2</span>
git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
cd cluster/examples/kubernetes/ceph

1. kubectl create -f common.yaml
2. kubectl create -f operator.yaml
3. kubectl create -f common-external.yaml
4. ConfigMap/Secretリソースに外部のCeph clusterの情報としていれる
  - namespace
  - FSID
  - client admin
  - monitor endpoint
5. kubectl create -f cluster-external.yaml
</code></pre></div><p>クラスター構築後は</p><ul><li>local: OSD / MON / MGR は存在しない</li><li>External: State が、Created ではなく、Connected の状態になる</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=12 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=2-rook-ceph-external-cluster-の利用方法>2. Rook-Ceph External Cluster の利用方法</h5><hr><ul><li>ストレージリソースを用意する際、local cluster 側の操作のみで完結できるのはよき。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=3-rook-ceph-external-cluster-利用時の注意点>3. Rook-Ceph External Cluster 利用時の注意点</h5><hr><ul><li>1 番気をつけた方がいいのは、Ceph バージョンかな？</li><li>configmap の修正が必要なのは、嫌だね。（いずれに、新しく最新 version を構築する場合は関係なさそう。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=5f8ed15c3bcf4136a53d04ad4d74b9e6 data-slide=14 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=参考リンク>参考リンク</h5><ul><li><a href=https://rook.io/docs/rook/v1.2/ceph-cluster-crd.html#external-cluster>Rook Doc - Ceph Cluster CRD #External Cluster</a></li><li><a href=https://github.com/rook/rook/blob/master/design/ceph/ceph-external-cluster.md>GitHub - Rook and External Ceph Clusters</a></li><li><a href=https://github.com/rook/rook/issues/4816>GitHub Issues - External cluster details are not populated to configmap rook-ceph-csi-config</a></li></ul><h3 id=rook-ceph-でいろいろベンチマークとってみる-japan_rookhttpstwittercomjapan_rook>Rook-Ceph でいろいろベンチマークとってみる <a href=https://twitter.com/japan_rook>@japan_rook</a></h3><hr><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=rook-ceph-で-io-計測をする>Rook Ceph で IO 計測をする</h4><h5 id=モチベーション>モチベーション</h5><ul><li>IO 測るの楽しい！笑</li><li>ストレージ直と Rook-Ceph を挟むとどれくらい変わるか</li><li>構成変更による IO の変化</li></ul><h5 id=環境>環境</h5><ul><li>worker は、Rook-Ceph と IO をかけるが同居するため、割と強めにしている。</li><li>一応、現時点で最新 version の組み合わせ。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=5 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h5 id=遊び方>遊び方</h5><ol><li>FIO 3.13</li></ol><ul><li><a href=https://kubestone.io/en/latest/benchmarks/fio/>kubestone fio</a></li><li>fio の Custom Resource が便利</li><li>ここの<a href=https://kubestone.io/en/latest/quickstart/>Quick Start</a>通りにやれば ok ってことかな？</li></ul><ol start=2><li>fio の pod から 100GB をマウント</li><li>時間の都合で、4K random read, 4K random write のみ</li></ol><h5 id=何を測るか>何を測るか</h5><ol><li>素の EBS（gp2）vs Rook-Ceph 3x replica RBD</li><li>Ceph クラスタの OSD 数</li><li>レプリカ数（一般的に、3x の replicas だが。）</li></ol><h6 id=1-素の-ebsgp2vs-rook-ceph-3x-replica-rbd>1. 素の EBS（gp2）vs Rook-Ceph 3x replica RBD</h6><ul><li>負荷が低い時の write は結構違う。EBS の方が速い</li><li>個人的には、それぞれ OSD を 3 つつけていて、並列に read ができるので、1 個の EBS から read するのと、3 個の EBS から読み込むよりも有利なので、Rook-Ceph の方が役に立つ（read 強い。）</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=10 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=2-ceph-クラスタの-osd-数>2. Ceph クラスタの OSD 数</h6><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=13 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h6 id=3-レプリカ数>3. レプリカ数</h6><ul><li>clush rule にしたがって、平均になるように、replica されている</li><li>なので、利用される EBS の数は変わらない。</li></ul><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=16 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=まとめ>まとめ</h4><div class=embed><script async class=speakerdeck-embed data-id=815886a2378d49d6a10ae5bf29600e36 data-slide=17 data-ratio=1.77777777777777 src=//speakerdeck.com/assets/embed.js></script></div><h4 id=質疑応答-2>質疑応答</h4><blockquote><p>Q. MIN_SIZE（Ceph pool の最小サイズ？）は同じにしているのかな？</p><p>A. 同じ一にしています。2x でも死んでも計測し続けられるようにしています。</p></blockquote><blockquote><p>Q. best practice で、1 SSD 1 device あたり 〇〇 OSD に良いみたいな指針をみた気がするのですが。そういう指針に沿うと良いとか有りますか？</p><p>A. 有効に使う 1 SSD に対して、1 OSD にしても、あまり IO がこなければもったいないので、2 とか 4OSD にした方が良いという話もあります。感覚としては、一旦、1 SSD に対して、1~2 OSD が良いかなと。NVMe みたいに、並列に Queue がバシバシ入るものなら、4,6OSD でも良いかなと。もう、その辺はどの程度の負荷になるか。容量にするか。によって、変えてもらえればと思います。</p></blockquote><h2 id=さいごに>さいごに</h2><p>オンラインイベント初だったが、発表の音声ログなど残っていれば、そんなに問題ないなと思ったりしました。</p><div class=related><h3>Similar articles:</h3><ul><li><a href=/post/202002051855/>Terraform meetup tokyo#4</a></li><li><a href=/post/202001291937/>Kubernetes Meetup Tokyo #27</a></li><li><a href=/post/202001231917/>DMM.go #1</a></li><li><a href=/post/202001191310/>iOS・SwiftプロジェクトのCI/CDで、bitrise.ymlをリポジトリ内で管理する</a></li><li><a href=/post/202001150046/>Goもくもく会（ごもくかい）#23</a></li></ul></div></div></div></section><script src=/js/copycode.js></script><section class=section><div class="container has-text-centered"><p>(C) <a href=https://github.com/jkkitakita>Jun Kitamura</a> 2020</p></div></section><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-156067080-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript>var _paq=_paq||[];_paq.push(['trackPageView']);_paq.push(['enableLinkTracking']);(function(){var u="\/\/matomo.example.com\/";_paq.push(['setTrackerUrl',u+'piwik.php']);_paq.push(['setSiteId','1']);var d=document,g=d.createElement('script'),s=d.getElementsByTagName('script')[0];g.type='text/javascript';g.async=true;g.defer=true;g.src=u+'piwik.js';s.parentNode.insertBefore(g,s);})();</script><noscript><img src="//matomo.example.com/piwik.php?idsite=1&rec=1" style=border:0 alt></noscript></body></html>